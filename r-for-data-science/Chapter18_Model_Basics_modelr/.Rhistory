rm(list=ls())
# The goal of a model is to provide a simple low-dimensional sum‐
# mary of a dataset. Ideally, the model will capture true “signals” (i.e.,
# patterns generated by the phenomenon of interest), and ignore
# “noise” (i.e., random variation that you’re not interested in). Here we
# only cover “predictive” models, which, as the name suggests, gener‐
# ate predictions.
# There is another type of model that we’re not going
# to discuss: “data discovery” models. These models don’t make
# predictions, but instead help you discover interesting relationships
# within your data. (These two categories of models are sometimes
# called supervised and unsupervised, but I don’t think that terminol‐
# ogy is particularly illuminating.)
# There is a pair of ideas that you must understand in order to do
# inference correctly:
# Each observation can either be used for exploration or confirmation, not both.
# You can use an observation as many times as you like for explo‐
# ration, but you can only use it once for confirmation. As soon as
# you use an observation twice, you’ve switched from confirma‐
# tion to exploration.
# This is necessary because to confirm a hypothesis you must use data
# independent of the data that you used to generate the hypothesis.
# Otherwise you will be overoptimistic. There is absolutely nothing
# wrong with exploration, but you should never sell an exploratory
# analysis as a confirmatory analysis because it is fundamentally mis‐
# leading.
# If you are serious about doing a confirmatory analysis, one
# approach is to split your data into three pieces before you begin the
# analysis:
# 60% goes into a training (or visualization) set. You are allowed to do anything with this
# set: explore, fit tons of models, etc.
# 20% goes into a query set. You can use this data to compare
# models or visualizations by hand, but you’re not allowed to use
# it as part of an automated process.
# 20% is held back for a test set. You can only use this data ONCE,
# to test your final model.
# This partitioning allows you to explore the training data, occasion‐
# ally generating candidate hypotheses that you check with the query
# set. When you are confident you have the right model, you can
# check it once with the test data.
# ---- modelr() INTRODUCTION ---------------------------------------------------------- #
# The goal of a model is to provide a simple low-dimensional sum‐
# mary of a dataset. In the context of this book we’re going to use
# models to partition data into patterns and residuals. Strong patterns
# will hide subtler trends, so we’ll use models to help peel back layers
# of structure as we explore a dataset.
# There are two parts to a model:
# 1. First, you define a family of models that express a precise, but
# generic, pattern that you want to capture. For example, the pat‐
# tern might be a straight line, or a quadatric curve. You will
# express the model family as an equation like y = a_1 * x +
# a_2 or y = a_1 * x ^ a_2. Here, x and y are known variables
# from your data, and a_1 and a_2 are parameters that can vary to
# capture different patterns
# 2. Next, you generate a ftted model by finding the model from the
# family that is the closest to your data. This takes the generic
# model family and makes it specific, like y = 3 * x + 7 or y =
# 9 * x ^ 2.
# It’s important to understand that a fitted model is just the closest
# model from a family of models. That implies that you have the
# “best” model (according to some criteria); it doesn’t imply that you
# have a good model and it certainly doesn’t imply that the model is
# “true.” George Box puts this well in his famous aphorism:
georgebox <-  "All models are wrong, but some are useful."
georgebox
# In this chapter we’ll use the modelr package, which wraps around
# base R’s modeling functions to make them work naturally in a pipe.
library(tidyverse)
library(ggplot2)
# install.packages("modelr")
library(modelr)
options(na.action = na.warn)
ggplot(sim1, aes(x, y)) +
geom_point()
# Let’s take a look at the simulated dataset sim1. It contains two con‐
# tinuous variables, x and y. Let’s plot them to see how they’re related:
str(sim1)
models <- tibble(
a1 = runif(250, -20, 40)
a2 = runif(250, -5, 5))
models <- tibble(
a1 = runif(250, -20, 40),
a2 = runif(250, -5, 5))
models
aes(intercept = a1, slope = a2),
data = models, alpha = 1/4) +
geom_point()
ggplot(sim1, aes(x, y)) +
geom_abline(
aes(intercept = a1, slope = a2),
data = models, alpha = 1/4) +
geom_point()
a[1] + data$x * a[2]
model1 <- function(a, data){
a[1] + data$x * a[2]
}
model1(c(7, 1.5), sim1)
sqrt(mean(diff^2))
sqrt(mean(diff ^ 2))
sqrt(mean(I(diff ^ 2)))
sqrt(mean(diff ^ 2))
measure_distance(c(7, 1.5), sim1)
?math
?Math
?exp
sqrt(mean(as.numeric(diff) ^ 2))
?pow
?power
?exp
sqrt(mean(exp(diff, 2)))
sqrt(mean(diff ^2)
}
measure_distance(c(7, 1.5), sim1)
sqrt(mean(diff ^2))
measure_distance <- function(mod, data) {
diff <- data$y - model1(mod, data)
sqrt(mean(diff^2))
}
measure_distance(c(7, 1.5), sim1)
measure_distance(c(a1, a2), sim1)
sim1_dist <- function(a1, a2){
measure_distance(c(a1, a2), sim1)
}
models <- models %>%
mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
ggplot(sim1, aes(x, y)) +
geom_point(size = 2, color = "grey30") +
geom_abline(
aes(intercept = a1, slope = a2, color = -dist),
data = filter(models, rank(dist) <= 10)
)
data = filter(models, rank(dist) <= 10),
size = 4, color = "red") +
geom_point(aes(color = -dist))
ggplot(models, aes(a1, a2)) +
geom_point(
data = filter(models, rank(dist) <= 10),
size = 4, color = "red") +
geom_point(aes(color = -dist))
grid <- expand.grid(
a1 = seq(-5, 20, length = 25),
a2 = seq(1, 3, length = 25)) %>%
mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
grid %>%
ggplot(aes(a1, a2)) +
geom_point(
data = filter(grid, rank(dist) <= 10),
size = 4, color = "red") +
geom_point(aes(color = -dist))
ggplot(sim1, aes(x, y)) +
geom_point(size = 2, color = "grey30") +
geom_abline(
aes(intercept = a1, slope = a2, color = -dist),
data = filter(grid, rank(dist) <= 10))
best <- optim(c(0,0), measure_distance, data = sim1)
best$par
best
ggplot(sim1, aes(x, y)) +
geom_point(size = 2, color = "grey30") +
geom_abline(intercept = best$par[1], slope = best$par[2])
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
summary(sim1_mod)
