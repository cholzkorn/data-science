rm(list=ls())

?sim1

# The goal of a model is to provide a simple low-dimensional sum-
# mary of a dataset. Ideally, the model will capture true "signals" (i.e.,
# patterns generated by the phenomenon of interest), and ignore
# "noise" (i.e., random variation that you're not interested in). Here we
# only cover "predictive" models, which, as the name suggests, gener-
# ate predictions.

# There is another type of model that we're not going
# to discuss: "data discovery" models. These models don't make
# predictions, but instead help you discover interesting relationships
# within your data. (These two categories of models are sometimes
# called supervised and unsupervised, but I don't think that terminol-
# ogy is particularly illuminating.)

# There is a pair of ideas that you must understand in order to do
# inference correctly:
  # Each observation can either be used for exploration or confirmation, not both.
  
  # You can use an observation as many times as you like for explo-
  # ration, but you can only use it once for confirmation. As soon as
  # you use an observation twice, you've switched from confirma-
  # tion to exploration.

# This is necessary because to confirm a hypothesis you must use data
# independent of the data that you used to generate the hypothesis.
# Otherwise you will be overoptimistic. There is absolutely nothing
# wrong with exploration, but you should never sell an exploratory
# analysis as a confirmatory analysis because it is fundamentally mis-
# leading.

# If you are serious about doing a confirmatory analysis, one
# approach is to split your data into three pieces before you begin the
# analysis:
  # 60% goes into a training (or visualization) set. You are allowed to do anything with this
  # set: explore, fit tons of models, etc.

  # 20% goes into a query set. You can use this data to compare
  # models or visualizations by hand, but you're not allowed to use
  # it as part of an automated process.

  # 20% is held back for a test set. You can only use this data ONCE,
  # to test your final model.

# This partitioning allows you to explore the training data, occasion-
# ally generating candidate hypotheses that you check with the query
# set. When you are confident you have the right model, you can
# check it once with the test data.












# ---- modelr() INTRODUCTION ---------------------------------------------------------- #

# The goal of a model is to provide a simple low-dimensional sum-
# mary of a dataset. In the context of this book we're going to use
# models to partition data into patterns and residuals. Strong patterns
# will hide subtler trends, so we'll use models to help peel back layers
# of structure as we explore a dataset.

# There are two parts to a model:
  # 1. First, you define a family of models that express a precise, but
  # generic, pattern that you want to capture. For example, the pat-
  # tern might be a straight line, or a quadatric curve. You will
  # express the model family as an equation like y = a_1 * x +
  # a_2 or y = a_1 * x ^ a_2. Here, x and y are known variables
  # from your data, and a_1 and a_2 are parameters that can vary to
  # capture different patterns

  # 2. Next, you generate a ftted model by finding the model from the
  # family that is the closest to your data. This takes the generic
  # model family and makes it specific, like y = 3 * x + 7 or y =
  # 9 * x ^ 2.

# It's important to understand that a fitted model is just the closest
# model from a family of models. That implies that you have the
# "best" model (according to some criteria); it doesn't imply that you
# have a good model and it certainly doesn't imply that the model is
# "true." George Box puts this well in his famous aphorism:

georgebox <-  "All models are wrong, but some are useful."
georgebox

# In this chapter we'll use the modelr package, which wraps around
# base R's modeling functions to make them work naturally in a pipe.

library(tidyverse)
library(ggplot2)
# install.packages("modelr")
library(modelr)
options(na.action = na.warn)









# ---- A SIMPLE MODEL --------------------------------------------------------------------- #

# Let's take a look at the simulated dataset sim1. It contains two con-
# tinuous variables, x and y. Let's plot them to see how they're related:
str(sim1)

ggplot(sim1, aes(x, y)) +
  geom_point()

# Let's start by getting a feel for what models from that
# family look like by randomly generating a few and overlaying them
# on the data. For this simple case, we can use geom_abline(), which
# takes a slope and intercept as parameters. Later on we'll learn more
# general techniques that work with any model:

models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5))

ggplot(sim1, aes(x, y)) +
  geom_abline(
    aes(intercept = a1, slope = a2),
    data = models, alpha = 1/4) +
  geom_point()

# There are 250 models on this plot, but a lot are really bad! We need
# to find the good models by making precise our intuition that a good
# model is "close" to the data. We need a way to quantify the distance
# between the data and a model. Then we can fit the model by finding
# the values of a_0 and a_1 that generate the model with the smallest
# distance from this data.

# One easy place to start is to find the vertical distance between each
# point and the model, as in the following diagram (BOOK!)

# This distance is just the difference between the y value given by the
# model (the prediction), and the actual y value in the data (the response).

# To compute this distance, we first turn our model family into an R
# function. This takes the model parameters and the data as inputs,
# and gives values predicted by the model as output:

model1 <- function(a, data){
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)

# Next, we need some way to compute an overall distance between the
# predicted and actual values. In other words, the plot shows 30 dis-
# tances: how do we collapse that into a single number?

# One common way to do this in statistics is to use the "root-meansquared deviation."
# We compute the difference between actual and
# predicted, square them, average them, and then take the square root.
# This distance has lots of appealing mathematical properties, which
# we're not going to talk about here. You'll just have to take my word for it!

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff^2))
}

measure_distance(c(7, 1.5), sim1)

# Now we can use purrr to compute the distance for all the models
# defined previously. We need a helper function because our distance
# function expects the model as a numeric vector of length 2:

sim1_dist <- function(a1, a2){
  measure_distance(c(a1, a2), sim1)
}

models <- models %>%
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

models

# Next, let's overlay the 10 best models on to the data. I've colored the
# models by -dist: this is an easy way to make sure that the best mod-
# els (i.e., the ones with the smallest distance) get the brighest colors:

ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "grey30") +
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist),
    data = filter(models, rank(dist) <= 10))

# We can also think about these models as observations, and visualize
# them with a scatterplot of a1 versus a2, again colored by -dist. We
# can no longer directly see how the model compares to the data, but
# we can see many models at once. Again, I've highlighted the 10 best
# models, this time by drawing red circles underneath them:

ggplot(models, aes(a1, a2)) +
  geom_point(
    data = filter(models, rank(dist) <= 10),
    size = 4, color = "red") +
  geom_point(aes(color = -dist))

##### GRID SEARCH

# Instead of trying lots of random models, we could be more system-
# atic and generate an evenly spaced grid of points (this is called a grid
# search). I picked the parameters of the grid roughly by looking at
# where the best models were in the preceding plot:

grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)) %>%
mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>%
  ggplot(aes(a1, a2)) +
    geom_point(
      data = filter(grid, rank(dist) <= 10),
      size = 4, color = "red") +
    geom_point(aes(color = -dist))


# When you overlay the best 10 models back on the original data, they
# all look pretty good:

ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "grey30") +
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist),
    data = filter(grid, rank(dist) <= 10))

# You could imagine iteratively making the grid finer and finer until
# you narrowed in on the best model. But there's a better way to tackle
# that problem: a numerical minimization tool called Newton-Raph-
# son search. The intuition of Newton-Raphson is pretty simple: you
# pick a starting point and look around for the steepest slope. You
# then ski down that slope a little way, and then repeat again and
# again, until you can't go any lower. In R, we can do that with
# optim():

best <- optim(c(0,0), measure_distance, data = sim1)
best$par

ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "grey30") +
  geom_abline(intercept = best$par[1], slope = best$par[2])

# There's one more approach that we can use for this model, because it
# is a special case of a broader family: linear models. A linear model
# has the general form y = a_1 + a_2 * x_1 + a_3 * x_2 + ... +
# a_n * x_(n - 1). So this simple model is equivalent to a general
# linear model where n is 2 and x_1 is x. R has a tool specifically
# designed for fitting linear models called lm(). lm() has a special way
# to specify the model family: formulas. Formulas look like y ~ x,
# which lm() will translate to a function like y = a_1 + a_2 * x. We
# can fit the model and look at the output:

sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
summary(sim1_mod)

# These are exactly the same values we got with optim()! Behind the
# scenes lm() doesn't use optim() but instead takes advantage of the
# mathematical structure of linear models. Using some connections
# between geometry, calculus, and linear algebra, lm() actually finds
# the closest model in a single step, using a sophisticated algorithm.
# This approach is faster and guarantees that there is a global mini-
# mum.





# ---- VISUALIZING MODELS ---------------------------------------------------------- #

# every type of predictive model makes predictions (otherwise what use would it be?)
# so we can use the same set of techniques to understand any type of predictive model.

# It's also useful to see what the model doesn't capture, the so-called
# residuals that are left after subtracting the predictions from the data.
# Residuals are powerful because they allow us to use models to
# remove striking patterns so we can study the subtler trends that remain.






# ---- PREDICTIONS ----------------------------------------------------------------- #

# To visualize the predictions from a model, we start by generating an
# evenly spaced grid of values that covers the region where our data
# lies. The easiest way to do that is to use modelr::data_grid(). 

# Its first argument is a data frame, and for each subsequent argument it
# finds the unique variables and then generates all combinations:

grid <- sim1 %>%
  data_grid(x)

grid

# (This will get more interesting when we start to add more variables to our model.)

# Next we add predictions. We'll use modelr::add_predictions(),
# which takes a data frame and a model. It adds the predictions from
# the model to a new column in the data frame:

coef(sim1_mod)

grid <- grid %>%
  add_predictions(sim1_mod)

grid

# (You can also use this function to add predictions to your original dataset.)

# Next, we plot the predictions. You might wonder about all this extra
# work compared to just using geom_abline(). But the advantage of
# this approach is that it will work with any model in R, from the sim-
# plest to the most complex. You're only limited by your visualization
# skills. For more ideas about how to visualize more complex model
# types, you might try http://vita.had.co.nz/papers/model-vis.html.

ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred),
            data = grid,
            color = "red",
            size= 1)



# ---- RESIDUALS ------------------------------------------------------------------ #

# The flip side of predictions are residuals. The predictions tell you the
# pattern that the model has captured, and the residuals tell you what
# the model has missed. The residuals are just the distances between
# the observed and predicted values that we computed earlier.

# We add residuals to the data with add_residuals(), which works
# much like add_predictions(). Note, however, that we use the origi-
# nal dataset, not a manufactured grid. This is because to compute
# residuals we need actual y values:

sim1_mod

sim1 <- sim1 %>%
  add_residuals(sim1_mod)
sim1


# A frequency polygon might help us understand the spread of the residuals

ggplot(sim1, aes(resid)) +
  geom_freqpoly(binwidth = 0.5)

# This helps you calibrate the quality of the model: how far away are
# the predictions from the observed values? Note that the average of
# the residual will always be 0.

# You'll often want to re-create plots using the residuals instead of the
# original predictor. You'll see a lot of that in the next chapter:

ggplot(sim1, aes(x, resid)) +
  geom_ref_line(h = 0) +
  geom_point()






###### EXERCISES #######

# Instead of using lm() to fit a straight line, you can use loess()
# to fit a smooth curve. Repeat the process of model fitting, grid
# generation, predictions, and visualization on sim1 using
# loess() instead of lm(). How does the result compare to geom_smooth()?

sim1_loess <- sim1
sim1_mod_loess <- loess(y ~ x, data = sim1)

# create grid

grid_loess <- sim1 %>%
  data_grid(x)

grid_loess

# add predictions

grid_loess <- grid_loess %>%
  add_predictions(sim1_mod_loess)

grid_loess

# add residuals

sim1_loess <- sim1_loess %>%
  add_residuals(sim1_mod_loess)
sim1_loess

# frequency polygon

ggplot(sim1_loess, aes(resid)) +
  geom_freqpoly(binwidth = 0.5)

# compare the models
head(cbind(sim1["x"], grid["pred"], grid_loess["pred"]))
as.tibble(cbind(sim1["x"], grid["pred"], grid_loess["pred"]))

comparison<- tibble(x = as.double(unlist(grid["x"])),
                    pred_lm = as.double(unlist(grid["pred"])),
                    pred_loess = as.double(unlist(grid_loess["pred"])))

comparison







# ---- FORMULAS AND MODEL FAMILIES ------------------------------------------------------ #

# The majority of modeling functions in R use a standard conversion
# from formulas to functions. You've seen one simple conversion
# already: y ~ x is translated to y = a_1 + a_2 * x. If you want to
# see what R actually does, you can use the model_matrix() function.

# It takes a data frame and a formula and returns a tibble that defines
# the model equation: each column in the output is associated with
# one coefficient in the model, and the function is always y = a_1 *
# out1 + a_2 * out_2. For the simplest case of y ~ x1 this shows us
# something interesting:

df <- tribble(
  ~y, ~x1, ~x2,
  4, 2, 5,
  5, 1, 6)

model_matrix(df, y ~ x1)

# The way that R adds the intercept to the model is just by having a
# column that is full of ones. By default, R will always add this col-
# umn. If you don't want that, you need to explicitly drop it with -1:

model_matrix(df, y ~ x1 - 1)

# The model matrix grows in an unsurprising way when you add
# more variables to the model:

model_matrix(df, y ~ x1 + x2)

# This formula notation is sometimes called "Wilkinson-Rogers nota-
# tion," and was initially described in Symbolic Description of Factorial
# Models for Analysis of Variance, by G. N. Wilkinson and C. E. Rog-
# ers. It's worth digging up and reading the original paper if you'd like
# to understand the full details of the modeling algebra.
# The following sections expand on how this formula notation works
# for categorcal variables, interactions, and transformation.







# ---- CATEGORICAL VARIABLES ---------------------------------------------------------- #

# Imagine you have a formula like y ~ sex, where sex could either be male or female.
# It doesn't make sense to convert that to a formula like y = x_0 + x_1 * sex because sex
# isn't a number-you can't multiply it! Instead what R does is convert
# it to y = x_0 + x_1 * sex_male where sex_male is one if sex is
# male and zero otherwise:

df <- tribble(
  ~ sex, ~ response,
  "male", 1,
  "female", 2,
  "male", 1)

model_matrix(df, response ~ sex)

# since sexfemale would be a linear transformation of sexmale, R doesn't create this column

# Fortunately, however, if you focus on visualizing predictions you
# don't need to worry about the exact parameterization. Let's look at
# some data and models to make that concrete. Here's the sim2 dataset
# from modelr:

ggplot(sim2) +
  geom_point(aes(x, y))

# We can fit a model to it, and generate predictions:
mod2 <- lm(y ~ x, data = sim2)
summary(mod2)

grid <- sim2 %>%
  data_grid(x) %>%
  add_predictions(mod2)

grid

# Effectively, a model with a categorical x will predict the mean value
# for each category. (Why? Because the mean minimizes the root
# mean-squared distance.) That's easy to see if we overlay the predic-
# tions on top of the original data:

ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
  geom_point(
    data = grid,
    aes(y = pred),
    color = "red",
    size = 4)

# You can't make predictions about levels that you didn't observe.
# Sometimes you'll do this by accident so it's good to recognize this
# error message:

tibble(x = "e") %>%
  add_predictions(mod2)



# ---- INTERACTIONS (CONTINUOUS AND CATEGORICAL) ------------------------------------------ #

# What happens when you combine a continuous and a categorical
# variable? sim3 contains a categorical predictor and a continuous
# predictor. We can visualize it with a simple plot:

ggplot(sim3, aes(x1, y)) +
  geom_point(aes(color = x2))

# There are two possible models you could fit to this data:

mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

# When you add variables with +, the model will estimate each effect
# independent of all the others. It's possible to fit the so-called interac-
# tion by using *. For example, y ~ x1 * x2 is translated to y = a_0
# + a_1 * a1 + a_2 * a2 + a_12 * a1 * a2. Note that whenever
# you use *, both the interaction and the individual components are
# included in the model.

# To visualize these models we need two new tricks:
  # We have two predictors, so we need to give data_grid() both
  # variables. It finds all the unique values of x1 and x2 and then
  # generates all combinations.

  # To generate predictions from both models simultaneously, we
  # can use gather_predictions(), which adds each prediction as
  # a row. The complement of gather_predictions() is
  # spread_predictions(), which adds each prediction to a new
  # column.

grid <- sim3 %>%
  data_grid(x1, x2) %>%
  gather_predictions(mod1, mod2)

grid

## FACETING
# We can visualize the results for both models on one plot using faceting:

ggplot(sim3, aes(x1, y, color = x2)) +
  geom_point() +
  geom_line(data = grid, aes(y = pred)) +
  facet_wrap(~ model)

# Note that the model that uses + has the same slope for each line, but
# different intercepts. The model that uses * has a different slope and
# intercept for each line.

# Which model is better for this data? We can take look at the residu-
# als. Here I've faceted by both model and x2 because it makes it easier
# to see the pattern within each group:

sim3 <- sim3 %>%
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, color = x2)) +
  geom_point() +
  facet_grid(model ~ x2)

# There is little obvious pattern in the residuals for mod2. The residuals
# for mod1 show that the model has clearly missed some pattern in b,
# and less so, but still present, is pattern in c, and d. You might wonder
# if there's a precise way to tell which of mod1 or mod2 is better. There
# is, but it requires a lot of mathematical background, and we don't
# really care. Here, we're interested in a qualitative assessment of
# whether or not the model has captured the pattern that we're interested in.







# ---- INTERACTIONS: TWO CONTINUOUS VARIABLES --------------------------------------------- #

# Let's take a look at the equivalent model for two continuous vari-
# ables. Initially things proceed almost identically to the previous
# example:

sim4
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

grid <- sim4 %>%
  data_grid(
    x1 = seq_range(x1, 5),
    x2 = seq_range(x2, 5)) %>%
  gather_predictions(mod1, mod2)

grid

# demonstration how seq_range() works:
test <- c(2, 2, 3, 3)
seq_range(test, n = 5)

# Note my use of seq_range() inside data_grid(). Instead of using
# every unique value of x, I'm going to use a regularly spaced grid of
# five values between the minimum and maximum numbers. It's prob-
# ably not super important here, but it's a useful technique in general.
# There are three other useful arguments to seq_range():
  # pretty = TRUE will generate a "pretty" sequence, i.e., something
  # that looks nice to the human eye. This is useful if you want to
  # produce tables of output:

seq_range(c(0.0123, 0.923423), n = 5)
seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)

  # trim = 0.1 will trim off 10% of the tail values. This is useful if
  # the variable has a long-tailed distribution and you want to focus
  # on generating values near the center:

x1 <- rcauchy(100)
seq_range(x1, n = 5)
seq_range(x1, n = 5, trim = 0.10)
seq_range(x1, n = 5, trim = 0.25)
seq_range(x1, n = 5, trim = 0.50)

  # expand = 0.1 is in some sense the opposite of trim(); it
  # expands the range by 10%:

x2 <- c(0, 1)
seq_range(x2, n = 5)

# Next let's try and visualize that model. We have two continuous pre-
# dictors, so you can imagine the model like a 3D surface. We could
# display that using geom_tile():

ggplot(grid, aes(x1, x2)) +
  geom_tile(aes(fill = pred)) +
  facet_wrap(~ model)

# That doesn't suggest that the models are very different! But that's
# partly an illusion: our eyes and brains are not very good at accu-
# rately comparing shades of color. Instead of looking at the surface
# from the top, we could look at it from either side, showing multiple
# slices:

ggplot(grid, aes(x1, pred, color = x2, group = x2)) +
  geom_line() +
  facet_wrap(~ model)

ggplot(grid, aes(x2, pred, color = x1, group = x1)) +
  geom_line() +
  facet_wrap(~ model)

# This shows you that interaction between two continuous variables
# works basically the same way as for a categorical and continuous
# variable. An interaction says that there's not a fixed offset: you need
# to consider both values of x1 and x2 simultaneously in order to predict y.

# Notice that visualizing models with multiple predictors is hard!







# ---- TRANSFORMATIONS ----------------------------------------------------------- #

# You can also perform transformations inside the model formula. For
# example, log(y) ~ sqrt(x1) + x2 is transformed to y = a_1 +
# a_2 * x1 * sqrt(x) + a_3 * x2. If your transformation involves
# +, *, ^, or -, you'll need to wrap it in I() so R doesn't treat it like part
# of the model specification. For example, y ~ x + I(x ^ 2) is trans-
# lated to y = a_1 + a_2 * x + a_3 * x^2. If you forget the I() and
# specify y ~ x ^ 2 + x, R will compute y ~ x * x + x. x * x
# means the interaction of x with itself, which is the same as x. R auto-
# matically drops redundant variables so x + x becomes x, meaning
# that y ~ x ^ 2 + x specifies the function y = a_1 + a_2 * x.
# That's probably not what you intended!

# Again, if you get confused about what your model is doing, you can
# always use model_matrix() to see exactly what equation lm() is fitting:

df <- tribble(
  ~y, ~x,
  1, 1,
  2, 2,
  3, 3)

model_matrix(df, y ~ x^2 + x)
model_matrix(df, y ~ I(x^2) + x)

# Transformations are useful because you can use them to approxi-
# mate nonlinear functions. If you've taken a calculus class, you may
# have heard of Taylor's theorem, which says you can approximate any
# smooth function with an infinite sum of polynomials. That means
# you can use a linear function to get arbitrarily close to a smooth
# function by fitting an equation like y = a_1 + a_2 * x + a_3 *
# x^2 + a_4 * x ^ 3. Typing that sequence by hand is tedious, so R
# provides a helper function, poly():

model_matrix(df, y ~ poly(x, 2))

# However there's one major problem with using poly(): outside the
# range of the data, polynomials rapidly shoot off to positive or nega-
# tive infinity. One safer alternative is to use the natural spline,
# splines::ns():

library(splines)
model_matrix(df, y ~ ns(x, 2))

# Let's see what that looks like when we try and approximate a non
# linear function:

sim5 <- tibble(
  x = seq(0, 3.5 * pi, length = 50),
  y = 4 * sin(x) + rnorm(length(x)))

ggplot(sim5, aes(x, y)) +
  geom_point()

# I'm going to fit five models to this data:

mod1 <- lm(y ~ ns(x, 1), data = sim5)
mod2 <- lm(y ~ ns(x, 2), data = sim5)
mod3 <- lm(y ~ ns(x, 3), data = sim5)
mod4 <- lm(y ~ ns(x, 4), data = sim5)
mod5 <- lm(y ~ ns(x, 5), data = sim5)

grid <- sim5 %>%
  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>%
  gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = "y")

grid

ggplot(sim5, aes(x, y)) +
  geom_point() +
  geom_line(data = grid,  color = "red") +
  facet_wrap(~ model)

# Notice that the extrapolation outside the range of the data is clearly
# bad. This is the downside to approximating a function with a poly-
# nomial. But this is a very real problem with every model: the model
# can never tell you if the behavior is true when you start extrapolat-
# ing outside the range of the data that you have seen. You must rely
# on theory and science.









# ---- MISSING VALUES --------------------------------------------------------- #

# R's default behavior is to silently drop missing values, but options(na.action = na.warn)
# (run in the prerequesites), makes sure you get a warning:

options(na.action = na.warn)

df <- tribble(
  ~x, ~y,
  1, 2.2,
  2, NA,
  3, 3.5,
  4, 8.3,
  NA, 10)

mod <- lm(y ~ x, data = df)

# To suppress the warning, set na.action = na.exclude:

mod <- lm(y ~ x, data = df, na.action = na.exclude)

# You can always see exactly how many observations were used with nobs():

nobs(mod)









# ---- OTHER MODEL FAMILIES ----------------------------------------------------- #

# This chapter has focused exclusively on the class of linear models,
# which assume a relationship of the form y = a_1 * x1 + a_2 * x2
# + ... + a_n * xn. Linear models additionally assume that the
# residuals have a normal distribution, which we haven't talked about.

# There is a large set of model classes that extend the linear model in
# various interesting ways. Some of them are:

  # Generalized linear models, e.g., stats::glm(). Linear models
  # assume that the response is continuous and the error has a nor-
  # mal distribution. Generalized linear models extend linear mod-
  # els to include noncontinuous responses (e.g., binary data or
  # counts). They work by defining a distance metric based on the
  # statistical idea of likelihood.

?stats::glm()

  # Generalized additive models, e.g., mgcv::gam(), extend general-
  # ized linear models to incorporate arbitrary smooth functions.
  # That means you can write a formula like y ~ s(x), which
  # becomes an equation like y = f(x), and let gam() estimate what
  # that function is (subject to some smoothness constraints to
  # make the problem tractable)

?mgcv::gam()

  # Penalized linear models, e.g., glmnet::glmnet(), add a penalty
  # term to the distance that penalizes complex models (as defined
  # by the distance between the parameter vector and the origin).
  # This tends to make models that generalize better to new datasets
  # from the same population.

#install.packages("glmnet")
library(glmnet)
?glmnet::glmnet()

  # Robust linear models, e.g., MASS:rlm(), tweak the distance to
  # downweight points that are very far away. This makes them less
  # sensitive to the presence of outliers, at the cost of being not
  # quite as good when there are no outliers.

?MASS::rlm()

  ## TREES, RANDOM FORESTS AND BOOSTING
  # Trees, e.g., rpart::rpart(), attack the problem in a completely
  # different way than linear models. They fit a piece-wise constant
  # model, splitting the data into progressively smaller and smaller
  # pieces. Trees aren't terribly effective by themselves, but they are
  # very powerful when used in aggregate by models like random
  # forests (e.g., randomForest::randomForest()) or gradient boost-
  # ing machines (e.g., xgboost::xgboost.)

#install.packages("randomForest")
#install.packages("xgboost")

library(randomForest)
library(xgboost)

?rpart::rpart
?randomForest::randomForest()
?xgboost::xgboost
